{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61af9aaf-bc94-45c2-b104-92f06de8e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a61d7cb-d3b7-4b40-8715-7735f8875d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000209E10F9540>\n",
      "<SparkContext master=local[*] appName=df-project>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('df-project').getOrCreate()\n",
    "print(spark)\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853dfaa2-c53c-416c-8297-1f3485fe385c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x00000209E10F9540>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aa2fc21-d8e9-4601-ba32-c52b0adfe761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ca9cb3-90b9-464a-b72b-954766355e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#range function creates the dataframe with start , end , step\n",
    "df = spark.range(1,10,2)\n",
    "type(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898af64f-4a1e-406c-9d45-e3820034f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will create df with 1- 19\n",
    "df = spark.range(20)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771322b9-343f-407d-bf4d-f62e5ddcb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "+---+\n",
      "\n",
      "4\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "+---+\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#Getting num partition in range function\n",
    "df = spark.range(1,20,2)\n",
    "df.show()\n",
    "print(str(df.rdd.getNumPartitions()))\n",
    "\n",
    "#Setting the num partitions through function\n",
    "df = spark.range(1,20,2,numPartitions=6)\n",
    "df.show()\n",
    "print(str(df.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be37ac96-1ba1-4ae3-9b82-5e124a9f2d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DF without the schema\n",
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "| Ram| 23|\n",
      "|sham| 34|\n",
      "+----+---+\n",
      "\n",
      "Creating DF with the schema\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "| Ram| 23|\n",
      "|sham| 34|\n",
      "+----+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "| Ram| 23|\n",
      "|sham| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lst = [(\"Ram\",23),(\"sham\",34)]\n",
    "#Creating the dataFrame from the list of tuple\n",
    "print('Creating DF without the schema')\n",
    "df = spark.createDataFrame(lst)\n",
    "df.show()\n",
    "\n",
    "#Creating the dataFram from the list and schema \n",
    "print('Creating DF with the schema')\n",
    "schema = ['Name','Age']\n",
    "df = spark.createDataFrame(lst,schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "#Passing Data to the schema variable\n",
    "\n",
    "df = spark.createDataFrame(lst,schema=('Name string, Age int'))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eba040e-0f4b-436c-a5bf-1d0475eef74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 34| Ram|\n",
      "| 35|Sham|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Datafram from dict\n",
    "dict = [{\"name\":\"Ram\",'age':34},{\"name\":\"Sham\",'age':35}]\n",
    "df = spark.createDataFrame(dict)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d47468-344a-4eeb-829a-2573229c2cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+----+---+\n",
      "|  _1| _2|\n",
      "+----+---+\n",
      "| Ram| 23|\n",
      "|sham| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DF from RDD\n",
    "lst = [(\"Ram\",23),(\"sham\",34)]\n",
    "rdd = sc.parallelize(lst)\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39a8deaa-dcd0-401e-a1c1-462b40e4a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ram| 23|\n",
      "| ram| 23|\n",
      "| ram| 23|\n",
      "+----+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating Df using ROW\n",
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(name='ram',age=23),Row(name='ram',age=23),Row(name='ram',age=23)])\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91af1260-ad9c-4e53-80f7-5735f4bb4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "| ram| 23|\n",
      "|sham| 45|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the DF using pandas\n",
    "import pandas as pd \n",
    "#Creating the nested list \n",
    "lst = [['ram',23],['sham',45]]\n",
    "data_frame = pd.DataFrame(data=lst,columns=['Name','Age'])\n",
    "df = spark.createDataFrame(data_frame)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48df9c99-1501-41dd-8bc8-edc60bb32f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|age|\n",
      "+----+---+\n",
      "| ram| 34|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|Name|age|\n",
      "+----+---+\n",
      "| ram| 34|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running SQL query using the spark\n",
    "lst=[('ram',34),('sham',45)]\n",
    "df=spark.createDataFrame(lst,['Name','age'])\n",
    "#Creating the session view\n",
    "df.createOrReplaceTempView(\"emp\")\n",
    "spark.sql(\"\"\" select * from emp where Name='ram' \"\"\").show()\n",
    "\n",
    "#Creatig the global view\n",
    "df.createOrReplaceGlobalTempView('emp_global')\n",
    "spark.sql(\"\"\" select * from global_temp.emp_global where Name='ram' \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66dc62c9-31ef-4a8c-99ee-2196de5a2ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|age|\n",
      "+----+---+\n",
      "| ram| 34|\n",
      "|sham| 45|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the tables function\n",
    "lst=[('ram',34),('sham',45)]\n",
    "df=spark.createDataFrame(lst,['Name','age'])\n",
    "df.createOrReplaceTempView(\"emp\")\n",
    "df1 = spark.table(\"emp\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e948241-e976-4f3f-99fe-6adc06b4aee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
